ğŸŒŠ Open-Vocabulary Object Detection for Autonomous Surface Vessels (ASVs)

Computer Vision â€¢ Open-Vocabulary Detection â€¢ Visionâ€“Language Models â€¢ YOLOv8 â€¢ OWL-ViT â€¢ GroundingDINO â€¢ Streamlit

This project implements an end-to-end maritime perception pipeline that compares closed-set object detection with open-vocabulary, language-driven detection for real-world Autonomous Surface Vessel (ASV) environments.

The system enables natural-language object queries (e.g., "floating debris", "small boat", "unknown obstacle") and evaluates how modern visionâ€“language models generalize to unseen maritime hazards, a critical limitation of traditional detectors.

ğŸš€ Key Features

Closed-set YOLOv8 baseline for maritime obstacle detection

Open-vocabulary detection using OWL-ViT and GroundingDINO

Natural-language promptâ€“based object grounding

Unified evaluation framework for cross-model comparison

Quantitative metrics + qualitative visualizations

Interactive Streamlit demo for language-query inference

Clean, modular repository aligned with industry standards

ğŸ§  Models Implemented
1. YOLOv8 (Closed-Set Baseline)

Supervised training on maritime obstacle annotations

Establishes reference performance under fixed label space

Strong localization but limited generalization to unseen objects

2. OWL-ViT (Open-Vocabulary Detection)

Visionâ€“language transformer

Zero-shot detection using text prompts

Enables category-free object discovery

3. GroundingDINO (Language-Grounded Detection)

Phrase-level grounding with bounding box localization

Handles free-form textual descriptions

Effective for ambiguous and novel maritime objects

ğŸ“Š Datasets Used

The project uses multiple real-world maritime datasets for training, validation, and evaluation.

Dataset files are not included in the repository.
Please download them from the official sources and place them under the data/ directory.

MaSTr1325
Maritime Surface Target Dataset (1,325 annotated images)
ğŸ”— Links:
- [MaSTr Images 512x384](https://box.vicos.si/borja/mastr1325_dataset/MaSTr1325_images_512x384.zip)
- [MaSTr Ground Truth Annotations](https://box.vicos.si/borja/mastr1325_dataset/MaSTr1325_masks_512x384.zip)

MODD â€“ Maritime Obstacle Detection Dataset
Real-world video frames with wakes, glare, and occlusions
ğŸ”— [MODD_Datasetv1.0](https://vision.fe.uni-lj.si/RESEARCH/modd/modd_dataset1.0.zip)

MODS â€“ Maritime Object Detection (Stereo) Dataset
Stereo maritime imagery (left camera used)
ğŸ”— [mods](https://vision.fe.uni-lj.si/public/mods/mods.zip)

The datasets are not merged into a single training set.
They are unified only at preprocessing and evaluation stages to ensure fair, controlled comparisons.

ğŸ“‚ Project Structure
.
â”œâ”€â”€ notebooks/                         # End-to-end experiment notebooks
â”‚   â”œâ”€â”€ 01_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 02_yolov8_baseline.ipynb
â”‚   â”œâ”€â”€ 03_owlvit_groundingdino.ipynb
â”‚   â”œâ”€â”€ 04_evaluation_visualization.ipynb
â”‚   â””â”€â”€ 05_language_query_demo.ipynb
â”‚
â”œâ”€â”€ scripts/                           # Reusable utilities & demo
â”‚   â”œâ”€â”€ coco_conversion.py
â”‚   â”œâ”€â”€ evaluate_metrics.py
â”‚   â”œâ”€â”€ visualize_results.py
â”‚   â””â”€â”€ app_streamlit.py               # Streamlit UI
â”‚
â”œâ”€â”€ models/                            # Saved outputs & qualitative results
â”‚   â”œâ”€â”€ yolo_visuals/
â”‚   â”œâ”€â”€ owlvit_visuals/
â”‚   â”œâ”€â”€ groundingdino_visuals/
â”‚   â””â”€â”€ language_demo/
â”‚
â”œâ”€â”€ runs/                              # YOLO training & validation artifacts
â”‚
â”œâ”€â”€ reports/                           # Final metrics & comparison tables
â”‚   â”œâ”€â”€ yolo/
â”‚   â”œâ”€â”€ owlvit/
â”‚   â”œâ”€â”€ groundingdino/
â”‚   â””â”€â”€ model_comparison/
â”‚
â”œâ”€â”€ results/                           # Auxiliary exported metrics
â””â”€â”€ LICENSE

ğŸ§ª End-to-End Pipeline

Data preprocessing & normalization

YOLOv8 closed-set training and evaluation

Open-vocabulary inference with OWL-ViT & GroundingDINO

Cross-model evaluation and visualization

Language-query-based interactive demo

Each stage is implemented as a standalone, reproducible notebook.

ğŸ¨ Streamlit Demo

Run the interactive language-query demo:

streamlit run scripts/app_streamlit.py


Demo features:

Upload maritime images

Enter free-form natural-language prompts

Compare detections across YOLO, OWL-ViT, and GroundingDINO

ğŸ“ˆ Evaluation Summary (High-Level)

Closed-set models excel at known obstacle classes but fail on novel objects

Open-vocabulary models generalize better but are sensitive to glare and scale

Language-grounded detection enables flexible, human-interpretable perception

Results highlight the trade-offs between precision, generalization, and interpretability

Detailed metrics and plots are available under reports/.

ğŸ›  Tech Stack

Python 3.10+

PyTorch

YOLOv8 (Ultralytics)

OWL-ViT

GroundingDINO

OpenCV

NumPy, Matplotlib

Streamlit

ğŸŒ Applications

Autonomous Surface Vessels (ASVs)

Maritime navigation & obstacle avoidance

Open-world robotic perception

Safety-critical autonomy systems

ğŸ“Œ Future Extensions

Multi-sensor fusion (camera + sonar / radar)

Temporal tracking of open-vocabulary detections

On-board deployment optimization

Expanded rare-object maritime datasets

ğŸ“ License

MIT License.
